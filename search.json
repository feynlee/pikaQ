[
  {
    "objectID": "terms.html",
    "href": "terms.html",
    "title": "Terms",
    "section": "",
    "text": "source\n\n\n\n NullValue ()\n\nNULL value for use in queries.\n\nsource\n\n\n\n\n Value (value)\n\nA simple wrapper for a value that will be used in a query.\n\nsource\n\n\n\n\n Term ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntest_eq(Value(2).get_sql(), '2')\ntest_eq(Value('abc').get_sql(), \"'abc'\")\ntest_eq(Value(2).as_('col1').get_sql(), '2 as col1')\ntest_eq(Value('abc').as_('col1').get_sql(), \"'abc' as col1\")\ntest_eq(Value([1,2,3]).get_sql(), '(1, 2, 3)')\ntest_eq(Value(['col1', 'col2', 'col3']).get_sql(), \"('col1', 'col2', 'col3')\")\ntest_eq(NullValue().get_sql(), 'NULL')\ntest_eq(NullValue().as_('col1').get_sql(), 'NULL as col1')\ntest_eq(Value(Value(2)).get_sql(), '2')"
  },
  {
    "objectID": "terms.html#value",
    "href": "terms.html#value",
    "title": "Terms",
    "section": "",
    "text": "source\n\n\n\n NullValue ()\n\nNULL value for use in queries.\n\nsource\n\n\n\n\n Value (value)\n\nA simple wrapper for a value that will be used in a query.\n\nsource\n\n\n\n\n Term ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntest_eq(Value(2).get_sql(), '2')\ntest_eq(Value('abc').get_sql(), \"'abc'\")\ntest_eq(Value(2).as_('col1').get_sql(), '2 as col1')\ntest_eq(Value('abc').as_('col1').get_sql(), \"'abc' as col1\")\ntest_eq(Value([1,2,3]).get_sql(), '(1, 2, 3)')\ntest_eq(Value(['col1', 'col2', 'col3']).get_sql(), \"('col1', 'col2', 'col3')\")\ntest_eq(NullValue().get_sql(), 'NULL')\ntest_eq(NullValue().as_('col1').get_sql(), 'NULL as col1')\ntest_eq(Value(Value(2)).get_sql(), '2')"
  },
  {
    "objectID": "terms.html#field",
    "href": "terms.html#field",
    "title": "Terms",
    "section": "Field",
    "text": "Field\n\nsource\n\nCriteria\n\n Criteria (this, op, other)\n\nConstructor for criteria from two terms and automatically adds parentheses in appropriate places.\n\nsource\n\n\nArithmeticExpression\n\n ArithmeticExpression (this, op, other)\n\nConstructor for arithmetic expressions from two terms and automatically adds parentheses in appropriate places.\n\nsource\n\n\nField\n\n Field (name)\n\nA simple wrapper for a field that will be used in a query. Quotes can be added to the field name by setting the quote_char parameter in get_sql() method.\n\nsource\n\n\nFieldBase\n\n FieldBase ()\n\nCollection of methods to convert to ArithmeticExpression and Criteria\n\na = Field('a')\nb = Field('b')\nc = Field('c')\n\n# Test Field\ntest_eq(a.get_sql(), 'a')\ntest_eq(c.as_('c1').get_sql(), 'c AS c1')\n\n# Test ArithmeticExpression\ntest_eq((c-1).as_('new_c').get_sql(), 'c1 - 1 AS new_c')\ntest_eq((a+1&lt;13).get_sql(), 'a + 1 &lt; 13')\ntest_eq(((a + 1)/3).get_sql(), '(a + 1) / 3')\ntest_eq(((a + 1)/(b - 4)).get_sql(), '(a + 1) / (b - 4)')\ntest_eq(((a + 1&gt;2) & ((b-1&lt;10) | (b&gt;23)) ).get_sql(), \n        'a + 1 &gt; 2 and (b - 1 &lt; 10 or b &gt; 23)')\ntest_eq((((a + 1&gt;2) & (b-1&lt;=10)) | (b&gt;100)).get_sql(), '(a + 1 &gt; 2 and b - 1 &lt;= 10) or b &gt; 100')\n\n# Test Criteria\ntest_eq(a.eq(b).get_sql(), 'a = b')\ntest_eq(a.ne(b).get_sql(), 'a &lt;&gt; b')\ntest_eq(a.gt(b).get_sql(), 'a &gt; b')\ntest_eq(a.ge(b).get_sql(), 'a &gt;= b')\ntest_eq(a.lt(b).get_sql(), 'a &lt; b')\ntest_eq(a.le(b).get_sql(), 'a &lt;= b')\ntest_eq(a.like(b).get_sql(), 'a LIKE b')\ntest_eq(a.not_like(b).get_sql(), 'a NOT LIKE b')\ntest_eq(a.ilike('%what').get_sql(), \"a ILIKE '%what'\")\ntest_eq(a.not_ilike('%hh%').get_sql(), \"a NOT ILIKE '%hh%'\")\ntest_eq(a.isin([2, 3, 5]).get_sql(), 'a IN (2, 3, 5)')\ntest_eq(a.notin([2, 3, 5]).get_sql(), 'a NOT IN (2, 3, 5)')\ntest_eq(a.isnull().get_sql(), 'a IS NULL')\ntest_eq(a.notnull().get_sql(), 'a IS NOT NULL')\n\n# Test negate\ntest_eq((a-1&gt;1).negate().get_sql(), 'NOT (a - 1 &gt; 1)')\n\n# Test quoted field\nd = Field('tbl.d')\ntest_eq(a.get_sql(quote_char='\"'), '\"a\"')\ntest_eq(d.get_sql(quote_char='\"'), '\"tbl\".\"d\"')\ntest_eq((d - 1).get_sql(quote_char='\"'), '\"tbl\".\"d\" - 1')\ntest_eq((d - 1 &gt; 2).get_sql(quote_char='\"'), '\"tbl\".\"d\" - 1 &gt; 2')\ntest_eq((d - 3).as_('new_d').get_sql(quote_char='\"'), '\"tbl\".\"d\" - 3 AS \"new_d\"')"
  },
  {
    "objectID": "terms.html#custom-functions",
    "href": "terms.html#custom-functions",
    "title": "Terms",
    "section": "Custom Functions",
    "text": "Custom Functions\n\nsource\n\ncustom_func\n\n custom_func (func=None, window_func=False, dialect=None)\n\nreturn Field\n\nsource\n\n\nDelayedFunc\n\n DelayedFunc (func, args, kwargs, window_func=True)\n\nDelay the execution of stored function until exec is run.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunc\n\n\n\n\n\nargs\n\n\n\n\n\nkwargs\n\n\n\n\n\nwindow_func\nbool\nTrue\nwhether this function is a window function\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nkwargs_func\n\n kwargs_func (func, *args, **kwargs)\n\nAllow arbitrary kwargs. Only pass those kwargs that are specified in func to func.\nFor functions that need to be parsed differently for different dialects, you can use the custom_func decorator.\n\n@custom_func\ndef add_months(column, num, dialect='sql'):\n    if dialect=='sql':\n        return f'DATE_ADD(month, {num}, {column})'\n    elif dialect=='snowflake':\n        return f'MONTH_ADD({column}, {num})'\n\n\ntest_eq((add_months(\"col1\", 3)-2 &gt; 2).get_sql(), 'DATE_ADD(month, 3, col1) - 2 &gt; 2')\ntest_eq((add_months(\"col1\", 3)-2 &gt; 2).get_sql(dialect='snowflake'), 'MONTH_ADD(col1, 3) - 2 &gt; 2')\n\nYou can even overwrite or extend the existing functions by using the custom_func decorator with specified dialects keyword argument.\n\n@custom_func(dialect='athena')\ndef add_months(column, num):\n    return f\"DATE_ADD('month', {num}, {column})\"\n\ntest_eq((add_months(\"col1\", 5).as_('new_date')).get_sql(dialect='athena'),\n        \"DATE_ADD('month', 5, col1) AS new_date\")"
  },
  {
    "objectID": "terms.html#window-clause",
    "href": "terms.html#window-clause",
    "title": "Terms",
    "section": "Window Clause",
    "text": "Window Clause\n\nsource\n\nFollowing\n\n Following (N=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPreceding\n\n Preceding (N=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nOverClause\n\n OverClause (expression)\n\nConstructor for OVER clause.\n\nsource\n\n\nDelayedFunc.over\n\n DelayedFunc.over (partition_by)\n\n\nsource\n\n\nArithmeticExpression.over\n\n ArithmeticExpression.over (partition_by)\n\n\ntest_eq(\n    OverClause('SUM(col1)').over('col0').orderby('col2').rows(Preceding(3), CURRENT_ROW).get_sql(), \n    'SUM(col1) OVER (PARTITION BY col0 ORDER BY col2 ROWS BETWEEN 3 PRECEDING AND CURRENT_ROW)')\n\n@custom_func(window_func=True)\ndef LAG(column, offset=1, default=None):\n    if default is None:\n        return f\"LAG({column}, {offset})\"\n    else:\n        return f\"LAG({column}, {offset}, {default})\"\n\ntest_eq(\n    LAG(Field('col2'), 1).over('col1').orderby('col3').range(Preceding(), Following(2)).get_sql(),\n    'LAG(col2, 1) OVER (PARTITION BY col1 ORDER BY col3 RANGE BETWEEN UNBOUNDED PRECEDING AND 2 FOLLOWING)'\n)"
  },
  {
    "objectID": "terms.html#case",
    "href": "terms.html#case",
    "title": "Terms",
    "section": "Case",
    "text": "Case\n\nsource\n\nCase\n\n Case ()\n\nConstructor for CASE statement.\n\ntest_eq(Case().when(Field('column1')&gt;3, True).else_(False).get_sql(), \n        'CASE\\nWHEN column1 &gt; 3 THEN True\\nELSE False\\nEND')\ntest_eq(Case().when(Field('column1')&gt;3, 1).when(Field('column1')&lt;1, -1).else_(0).as_('b').get_sql(), \n        'CASE\\nWHEN column1 &gt; 3 THEN 1\\nWHEN column1 &lt; 1 THEN -1\\nELSE 0\\nEND AS b')"
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "Functions",
    "section": "",
    "text": "source\n\n\n\n CustomFunction.over (partition_by)\n\n\nsource\n\n\n\n\n CustomFunction (func_name:str, arg_names:list, window_func=False,\n                 distinct_option=False)\n\nA convenient class for creating custom functions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunc_name\nstr\n\nname of the function\n\n\narg_names\nlist\n\nlist of arg names\n\n\nwindow_func\nbool\nFalse\nwhether this can be used as a window function\n\n\ndistinct_option\nbool\nFalse\nwhether this function can be used with the distinct option\n\n\nReturns\nNone\n\n\n\n\n\nCustomFunction is a convenient class to create a custom SQL function with the name, and positional arguments, if you donâ€™t need the function to be parsed differently for different dialects. If you want to implement a custom function that is parsed differently for different dialects, you can use the custom_function decorator defined in terms.\n\ndate_diff = CustomFunction('DATE_DIFF', ['interval', 'start_date', 'end_date'])\ntest_eq(date_diff('day', 'start_date', 'end_date').get_sql(), 'DATE_DIFF(day, start_date, end_date)')\n\n\ndate_diff = CustomFunction('DATE_DIFF', ['interval', 'start_date', 'end_date'])\ntest_eq(date_diff('month', Field('date1'), Field('date2')).get_sql(), 'DATE_DIFF(month, date1, date2)')\n\n\n\n\n\n\n\n\n MAX (field)\n\n\n\n\n\n\n MIN (field)\n\n\n\n\n\n\n SUM (field)\n\n\n\n\n\n\n AVG (field)\n\n\n\n\n\n\n COUNT (field)\n\n\n\n\n\n\n ABS (field)\n\n\n\n\n\n\n ROUND (field, decimals)\n\n\n\n\n\n\n FIRST (field)\n\n\n\n\n\n\n LAST (field)\n\n\n\n\n\n\n DateTrunc (interval, date, dialect='sql')\n\n\n\n\n\n\n DateDiff (interval, start_date, end_date, dialect='sql')\n\n\n\n\n\n\n AddMonths (date, months, dialect='sql')\n\n\n\n\n\n\n Date (expression, format=None, dialect='sql')\n\n\nsource\n\n\n\n\n convert_date_format (format, dialect='sql')\n\n\n\n\n\n\n Concat (*args)\n\n\n\n\n\n\n Coalesce (*args)\n\n\n\n\n\n\n Cast (field, type)\n\n\n\n\n\n\n MONTHS_BETWEEN (start_date, end_date)\n\n\n\n\n\n\n ROW_NUMBER ()\n\n\n\n\n\n\n RANK ()\n\n\n\n\n\n\n DENSE_RANK ()\n\n\n\n\n\n\n PERCENT_RANK ()\n\n\n\n\n\n\n CUME_DIST ()\n\n\n\n\n\n\n NTILE (num_buckets)\n\n\n\n\n\n\n LAG (field, offset)\n\n\n\n\n\n\n LEAD (field, offset)\n\n\n\n\n\n\n FIRST_VALUE (field)\n\n\n\n\n\n\n LAST_VALUE (field)\n\n\n\n\n\n\n NTH_VALUE (field, n)\n\nThose that can be used as window functions:\n\ntest_eq(Max(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'MAX(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Min(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'MIN(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Sum(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'SUM(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Avg(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'AVG(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Count(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'COUNT(col1) OVER (PARTITION BY col2 ORDER BY col3)')\n\ntest_eq(RowNumber().over(Field('col1')).orderby(Field('col2')).get_sql(), \n        'ROW_NUMBER() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(Rank().over(Field('col1')).orderby(Field('col2')).get_sql(), \n        'RANK() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(DenseRank().over(Field('col1')).orderby(Field('col2')).get_sql(),\n        'DENSE_RANK() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(PercentRank().over(Field('col1')).orderby(Field('col2')).get_sql(),\n        'PERCENT_RANK() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(CumeDist().over(Field('col1')).orderby(Field('col2')).get_sql(),\n        'CUME_DIST() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(Ntile(5).over(Field('col1')).orderby(Field('col2')).get_sql(),\n        'NTILE(5) OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(Lag(Field('col1'), 1).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'LAG(col1, 1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Lead(Field('col1'), 1).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'LEAD(col1, 1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(FirstValue(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'FIRST_VALUE(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(LastValue(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'LAST_VALUE(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(NthValue(Field('col1'), 2).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'NTH_VALUE(col1, 2) OVER (PARTITION BY col2 ORDER BY col3)')\n\nCount and Sum can have a .distinct method.\n\ntest_eq(Count(Field('col')).distinct().get_sql(), 'COUNT(DISTINCT col)')\ntest_eq(Sum(Field('col')).distinct().get_sql(), 'SUM(DISTINCT col)')"
  },
  {
    "objectID": "functions.html#commonly-used-functions",
    "href": "functions.html#commonly-used-functions",
    "title": "Functions",
    "section": "",
    "text": "MAX (field)\n\n\n\n\n\n\n MIN (field)\n\n\n\n\n\n\n SUM (field)\n\n\n\n\n\n\n AVG (field)\n\n\n\n\n\n\n COUNT (field)\n\n\n\n\n\n\n ABS (field)\n\n\n\n\n\n\n ROUND (field, decimals)\n\n\n\n\n\n\n FIRST (field)\n\n\n\n\n\n\n LAST (field)\n\n\n\n\n\n\n DateTrunc (interval, date, dialect='sql')\n\n\n\n\n\n\n DateDiff (interval, start_date, end_date, dialect='sql')\n\n\n\n\n\n\n AddMonths (date, months, dialect='sql')\n\n\n\n\n\n\n Date (expression, format=None, dialect='sql')\n\n\nsource\n\n\n\n\n convert_date_format (format, dialect='sql')\n\n\n\n\n\n\n Concat (*args)\n\n\n\n\n\n\n Coalesce (*args)\n\n\n\n\n\n\n Cast (field, type)\n\n\n\n\n\n\n MONTHS_BETWEEN (start_date, end_date)\n\n\n\n\n\n\n ROW_NUMBER ()\n\n\n\n\n\n\n RANK ()\n\n\n\n\n\n\n DENSE_RANK ()\n\n\n\n\n\n\n PERCENT_RANK ()\n\n\n\n\n\n\n CUME_DIST ()\n\n\n\n\n\n\n NTILE (num_buckets)\n\n\n\n\n\n\n LAG (field, offset)\n\n\n\n\n\n\n LEAD (field, offset)\n\n\n\n\n\n\n FIRST_VALUE (field)\n\n\n\n\n\n\n LAST_VALUE (field)\n\n\n\n\n\n\n NTH_VALUE (field, n)\n\nThose that can be used as window functions:\n\ntest_eq(Max(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'MAX(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Min(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'MIN(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Sum(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'SUM(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Avg(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'AVG(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Count(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'COUNT(col1) OVER (PARTITION BY col2 ORDER BY col3)')\n\ntest_eq(RowNumber().over(Field('col1')).orderby(Field('col2')).get_sql(), \n        'ROW_NUMBER() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(Rank().over(Field('col1')).orderby(Field('col2')).get_sql(), \n        'RANK() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(DenseRank().over(Field('col1')).orderby(Field('col2')).get_sql(),\n        'DENSE_RANK() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(PercentRank().over(Field('col1')).orderby(Field('col2')).get_sql(),\n        'PERCENT_RANK() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(CumeDist().over(Field('col1')).orderby(Field('col2')).get_sql(),\n        'CUME_DIST() OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(Ntile(5).over(Field('col1')).orderby(Field('col2')).get_sql(),\n        'NTILE(5) OVER (PARTITION BY col1 ORDER BY col2)')\ntest_eq(Lag(Field('col1'), 1).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'LAG(col1, 1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(Lead(Field('col1'), 1).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'LEAD(col1, 1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(FirstValue(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'FIRST_VALUE(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(LastValue(Field('col1')).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'LAST_VALUE(col1) OVER (PARTITION BY col2 ORDER BY col3)')\ntest_eq(NthValue(Field('col1'), 2).over(Field('col2')).orderby(Field('col3')).get_sql(),\n        'NTH_VALUE(col1, 2) OVER (PARTITION BY col2 ORDER BY col3)')\n\nCount and Sum can have a .distinct method.\n\ntest_eq(Count(Field('col')).distinct().get_sql(), 'COUNT(DISTINCT col)')\ntest_eq(Sum(Field('col')).distinct().get_sql(), 'SUM(DISTINCT col)')"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utilities",
    "section": "",
    "text": "source\n\nexecute\n\n execute (obj, **kwargs)\n\nCall the exec method of obj if it exists, otherwise return the string representation of obj\n\nsource\n\n\nto_sql\n\n to_sql (l)\n\n\nsource\n\n\nquote_symbol\n\n quote_symbol (quote)\n\ngenerate quote symbol to use for tables and columns\n\nl = [23, 42]\ntest_eq(to_sql(l), '(23, 42)')\nl = ['category1', 'category2']\ntest_eq(to_sql(l), \"('category1', 'category2')\")\n\n\nsource\n\n\ndelegates\n\n delegates (to)\n\nDecorator: replace func name, signature and docstring with toâ€™s."
  },
  {
    "objectID": "queries.html",
    "href": "queries.html",
    "title": "Queries",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "queries.html#extending-query",
    "href": "queries.html#extending-query",
    "title": "Queries",
    "section": "Extending Query",
    "text": "Extending Query\nWe can extend SelectQuery to support more complex queries. There are 3 things we need to implement: 1. A method to record all necessary information for the SQL clause. Letâ€™s call it custom for example. 2. A method to generate the SQL clause str. This methodâ€™s name has to be parse_ + the previous methodâ€™s name (parse_custom). 3. The class variable sql_keys has to be overwritten to include the new methodâ€™s name in the appropriate position. The order of the keys in this list determines the order of the SQL clauses in the final SQL str.\nFor example, for Snowflake SQL, to generate the PIVOT clause, we need to implement both a pivot method and a parse_pivot method, and also add pivot into sql_keys at the right place.\n\nclass SFSelectQuery(SelectQuery):\n    # the order to put together the final sql query\n    sql_keys = ['with', 'select', 'from', 'join', 'pivot', 'where', 'groupby', 'having', 'orderby', 'limit']\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def pivot(self, agg_func, pivot_col, value_col, pivot_values, alias):\n        self.dic['pivot'] = {\n            'agg_func': agg_func,\n            'pivot_col': pivot_col,\n            'value_col': value_col,\n            'pivot_values': pivot_values,\n            'alias': alias\n        }\n        return self\n\n    def parse_pivot(self, **kwargs):\n        dialect = kwargs.get('dialect', None)\n        if dialect == 'snowflake':\n            dic = self.dic['pivot']\n            agg_func = dic['agg_func']\n            pivot_col = dic['pivot_col']\n            value_col = dic['value_col']\n            pivot_values = dic['pivot_values']\n            alias = dic['alias']\n            pivot_values = ', '.join([f\"'{v}'\" for v in pivot_values])\n            return f\"pivot({execute(agg_func(value_col), **kwargs)} for {execute(pivot_col, **kwargs)} in ({pivot_values})) as {alias}\"\n        else:\n            raise NotImplementedError(f\"dialect {dialect} not implemented\")\n\nTo use this new SFSelectQuery in Query, we simply assign it to the class variable q. And we can now use .pivot in our query.\n\nQuery.q = SFSelectQuery\n\nvw = Table('vw')\nprint(\n    Query\n    .from_(vw)\n    .pivot(fn.Sum, vw.amount, vw.month, ['JAN', 'FEB', 'MAR', 'APR'], 'p')\n    .where(vw.column &gt; 1).select(vw.star()).get_sql(dialect='snowflake')\n)\n\nselect vw.*\nfrom vw\npivot(SUM(vw.month) for vw.amount in ('JAN', 'FEB', 'MAR', 'APR')) as p\nwhere vw.column &gt; 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pikaQ",
    "section": "",
    "text": "This library is heavily inspired by PyPika, so a lot of the syntax is based on how PyPika works. Just like PyPika, PikaQ replaces handwritten SQL queries with programmatic construction of queries. The main difference is that pikaQ is designed to generate different SQL dialects from the same code. This is done by using a common syntax that is then translated to the specific dialect.\nThis library provides the core components and implementation for constructing the Select query and the core mechanism for translating it into different dialect. It does not offer complete coverage of SQL syntax nor the detailed implementation of all the dialects. However, it should be easy to extend the library to support more SQL syntax and other types of queries you want to construct.\nValidation of SQL correctness is not an explicit goal of PikaQ. You are encouraged to check inputs you provide to PyPika or appropriately handle errors raised from your SQL database - just as you would have if you were writing SQL yourself."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "pikaQ",
    "section": "Install",
    "text": "Install\npip install pikaQ"
  },
  {
    "objectID": "index.html#delayed-translation",
    "href": "index.html#delayed-translation",
    "title": "pikaQ",
    "section": "Delayed Translation",
    "text": "Delayed Translation\nThe translation of the query is delayed until the get_sql method is called. All the components of the query have an execute method that returns the SQL string for that component, and when get_sql is called, the execute method of all the components are called recursively to generate the final SQL string.\nFor example, if we want to write the same query in Spark SQL and AWS Athena, we might encounter this problem: we have ADD_MONTHS function in Spark SQL, but in AWS Athena (Presto) we donâ€™t have this function. We can define an ADD_MONTHS function in the following way:\n\nfrom pikaQ.terms import custom_func\nfrom pikaQ.queries import Query\n\n\n@custom_func\ndef add_months(column, value, dialect='spark'):\n    if dialect == 'athena':\n        return f\"DATE_ADD('month', {value}, {column})\"\n    elif dialect == 'spark':\n        return f'ADD_MONTH({column}, {value})'\n    else:\n        raise ValueError(f'Unsupported dialect: {dialect}')\n\n\nq = (Query.from_('table')\n        .select('col1', add_months('col2', 3).as_('col2'))\n)\n\nThen we can generate the query for Spark SQL and AWS Athena:\n\nprint(q.get_sql(dialect='athena'))\n\nselect col1, DATE_ADD('month', 3, col2) AS col2\nfrom table\n\n\n\nprint(q.get_sql(dialect='spark'))\n\nselect col1, ADD_MONTH(col2, 3) AS col2\nfrom table"
  },
  {
    "objectID": "index.html#select-query",
    "href": "index.html#select-query",
    "title": "pikaQ",
    "section": "Select Query",
    "text": "Select Query\nA more complex example to show how the syntax works:\n\nfrom pikaQ.queries import Query, Table, Field, AliasedQuery\nfrom pikaQ.terms import Case, Preceding, CURRENT_ROW\nimport pikaQ.functions as fn\n\n\na = Table('tbl1').as_('a')\nb = Table('tbl2').as_('b')\ns = AliasedQuery('s')\nm = AliasedQuery('m')\nv = AliasedQuery('v')\n\nq0 = (Query\n      .from_(a)\n      .select(\n         a.col1,\n         a.col2,\n         fn.Sum(a.col3).over(b.col2).rows(Preceding(3), CURRENT_ROW).as_('total'), \n         fn.RowNumber().over(b.col2).orderby(b.col4).as_('row_num')\n      ).distinct()\n      .where((Field('col2')-100&gt;2) & (Field('col3')/9&lt;=1))\n      .orderby(b.col2)\n)\nq1 = (Query\n      .from_(b)\n      .select(b.col1, b.col2, fn.Avg(b.col3).as_('avg'))\n      .groupby(b.col1, b.col2)\n      .having(fn.Count(b.col3)&gt;2)\n)\n\nq = (Query\n     .with_(q0, 's')\n     .with_(Query\n         .from_(s)\n         .select(s.star())\n         .where(s.row_num == 1), 'm')\n     .with_(q1, 'v')\n     .from_('v')\n     .join('m')\n        .on(\n            (v.col1 == m.col1) &\n            (v.col2 == m.col2)\n            )\n     .select(\n         v.col1, v.col2, v.avg,\n         Case().when(m.total&gt;100, 1).else_(0).as_('flag')\n         )\n)\n\nprint(q.get_sql())\n\nwith s as (\nselect distinct a.col1, a.col2, SUM(a.col3) OVER (PARTITION BY b.col2 ROWS BETWEEN 3 PRECEDING AND CURRENT_ROW) AS total, ROW_NUMBER() OVER (PARTITION BY b.col2 ORDER BY b.col4) AS row_num\nfrom tbl1 as a\nwhere col2 - 100 &gt; 2 and col3 / 9 &lt;= 1\norder by b.col2)\n\n, m as (\nselect s.*\nfrom s\nwhere s.row_num = 1)\n\n, v as (\nselect b.col1, b.col2, AVG(b.col3)\nfrom tbl2 as b\ngroup by b.col1, b.col2\nhaving COUNT(b.col3) &gt; 2)\n\nselect v.col1, v.col2, v.avg, CASE\nWHEN m.total &gt; 100 THEN 1\nELSE 0\nEND AS flag\nfrom v\njoin m on v.col1 = m.col1 and v.col2 = m.col2\n\n\nFor more syntax examples, please refer to the docs."
  },
  {
    "objectID": "index.html#extension",
    "href": "index.html#extension",
    "title": "pikaQ",
    "section": "Extension",
    "text": "Extension\nOne can use the core components and logic implemented in this library to extend the functionality to support more SQL syntax and other types of queries. For details of how to extend the SelectQuery to support more clauses, please refer to the Extending Query section of the docs."
  }
]